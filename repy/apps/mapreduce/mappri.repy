# MapReduce for python/repy!
#
#
# TODO list:
#
#

# include parallelize

# !! currently only works for a single mapper !!
# i'll worry about splitting the data file up at a later time
def initalize_replicas(data):
    
    # generate strings:
    # generate primary location string
    prim_str = format_message(mycontext['my_addr'])

    # generate replica list to send
    replica_str = pad_int_to_str(len(mycontext['replica_list']), 4)
    replica_str += mycontext['replica_str']

    # generate data string to send - size limited to 1e6-1 bytes
    data = format_message(data)
    
    # send data to each replica
    for ip, port in mycontext['replica_list'].iteritems():
        socketobj = openconn(ip, port)
        print "sending primary: ", prim_str
        socketobj.send(prim_str)
        print "sending replicas: ", replica_str
        socketobj.send(replica_str)
        print "sending data: ", data
        socketobj.send(data)

#        mycontext['is_received'] = socketobj


def send_message(socketobj, data):
    data = str(len(data)) + "*" + data
    socketobj.send(data)


def format_message(data):
    return str(len(data)) + "*" + data


def recv_message(socketobj, initialread=2):
    buf = ""
    found = False
    
    while not found:
        buf += socketobj.recv(initialread)
        if "*" in buf:
            found = True

    parts = buf.partition("*")
    data_len = int(parts[0])
    
    data = parts[2]
    data += socketobj.recv(data_len - len(parts[2]))      
    return data



def pad_int_to_str(i, len):
    if i / (10**len) > 0:
        raise Exception("integer overflow; consider revising protocol") 
    return ('%0' + str(len) + "d") % i




def get_results(ip, port, sockobj, thiscommhandle, listencommhandle):
    
    # parse actual data, write to file
    buf = recv_message(sockobj)
    result_file = open("results.dat", "w")
    result_file.write(buf)
    result_file.close()


def print_usage():
    print """Usage: mappri.repy [options] <datafile> <replica-ip>:<replica-port>...
    -n               Do not parse the data file with \\t splitting key and value
    <datafile>       The data file to use in the map-reduce pipeline
    <replica>:<port> A list of the replica addresses to use for the job
"""
    exit()


if callfunc == 'initialize':
    mycontext['replica_list'] = {}

    if len(callargs) == 0: 
        print "* No arguments provided."
        print_usage()
    
    argpointer = 0
    port = 12346
    try:
        port = int(callargs[0])
        ip = getmyip()
        argpointer += 1
        
        if len(callargs) == 1:
            print "* No arguments provided"
            print_usage()

    except ValueError:
        ip = '127.0.0.1'

    mycontext['my_addr'] = ip + ":" + str(port)

    if callargs[argpointer] == '-n':
        # do something special here, it's a no-op atm
        pass

    try:
        data_file = open(callargs[argpointer], "r")
        data = data_file.read()
        data_file.close()
        argpointer += 1
    except IOError:
        print "* Error reading file (does it exist?)"
        print_usage()

    replica_list = callargs[argpointer:]
    if len(replica_list) == 0:
        print "* List of replicas not found."
        print_usage()

    mycontext['replica_str'] = ""
    for replica in replica_list:
        address = replica.partition(':')
        if not address[2]:
            print "* Port not found!"
            print_usage()
        mycontext['replica_list'][address[0]] = int(address[2])

        addr = address[0] + ":" + address[2]
        addr_len = pad_int_to_str(len(addr), 3)
        mycontext['replica_str'] += addr_len + "*" + addr

    initalize_replicas(data)
    

    # once the job has started, we need to listen for reporting back 
    # by the replicas (right now, just one handle/no parallelization)
    
    listencommhandle = waitforconn(ip, port, get_results) 
