
'''
- header at top of file : docstring
- header for each function (you already have the descriptions) : docstring
- more space between functions, 3-4 space lines
'''


# MapReduce for python/repy!
#

#begin include mapper.repy
# MapReduce for python/repy!
#

def map_func(key, value):
    toRet = []
    for word in value.split():
        output = (word, 1)
        toRet.append(output)
    return toRet
#end include mapper.repy

#begin include reducer.repy
# MapReduce for python/repy!
#

def reduce_func(key, values):
    toRet = []
    sum = 0
    for value in values:
        sum += int(value)

    return {key: sum}
#end include reducer.repy

#begin include hasher.repy
#

def hash_func(data):
    return ord(data[0])/8
#end include hasher.repy

def get_data(ip, port, socketobj, thiscommhandle, listencommhandle):
    """ Listens for connections on a well-defined port, imports data files """
    
    # get a list of all of our neighbors!
    mycontext['primary'] = recv_message(socketobj)
    print "got primary loc: ", mycontext['primary']

    # we need to know how many peers we have..
    mycontext['num_peers'] = int(socketobj.recv(4))
    print "got num_peers: ", mycontext['num_peers']

    mycontext['peers'] = []
    for i in range(mycontext['num_peers']):
        mycontext['peers'].append(recv_message(socketobj))

    # parse and save data file:
    buf = recv_message(socketobj)
    print "got data, ", buf
    dataobj = open("map_data.dat", "w")
    dataobj.write(buf)
    dataobj.close()

    # make sure each replica has the same order!
    mycontext['peers'].sort()
    print "got my peers: ", mycontext['peers']

    # which peer am I?  save this for future reference..
    address_str = mycontext['myip'] + ":" + str(mycontext['myport'])
    mycontext['my_peer_index'] = mycontext['peers'].index(address_str)        

    # start the peer_sockets 
    mycontext['peer_sockets'] = [""] * mycontext['num_peers']
        
    # destroy the listen socket as we're done initializing
    mycontext['state'] = 'Initialized'
    stopcomm(listencommhandle)


    
# 
def init_replica_sockets():
    # we only want to make one socket for each link, so let's be smart about it
    # and only make new connections to equal or higher indexed peers..
    peer_subset = mycontext['peers'][mycontext['my_peer_index']:]
    for peer in peer_subset:
        addr_parts = peer.partition(":")
        socketobj = openconn(addr_parts[0], int(addr_parts[2]))
        socketobj.send("hello")

        peer_index = mycontext['peers'].index(peer)

        mycontext['connection_lock'].acquire()
        mycontext['peer_sockets'][peer_index] = socketobj
        mycontext['peer_conn_left'] -= 1
        mycontext['connection_lock'].release()

        print "succesfully communicated with", peer



# listens for incoming tcp connections, establishes a port
def recv_init_replica_conn(ip, port, sockobj, thiscommhandle, listencommhandle):
    data = sockobj.recv(5)
    if not data == "hello":
        exit_prog("Received an unintelligible message from a peer %s:%d, %s" %
                  (ip, port, data))
    else:
        # save socket obj for later
        incoming_index = mycontext['peers'].index(ip+":"+str(port))

        mycontext['connection_lock'].acquire()
        mycontext['peer_sockets'][incoming_index] = sockobj

        # decrement our connection counter
        mycontext['peer_conn_left'] -= 1
        mycontext['connection_lock'].release()

        print "successfully receive init message from", ip+":"+str(port)


# listens for incoming map data
def recv_peer_map_data(ip, port, socketobj, thiscommhandle, listencommhandle):
    mycontext['connection_lock'].acquire()
    mycontext['reduce_data'].update(recv_message_dict(socketobj))
    mycontext['connection_lock'].release()



# Assumptions to make this simpler:
# - all this data fits in memory (<2 GB) in the variable map_result
# - data is stored in the files/string as "(key)(\t)(value)"  
def do_map():
    mycontext['state'] = "Mapping"

    data = open("map_data.dat", "r")

    map_result = []
    for line in data:
        line_parts = line.partition('\t')
        # I assume that results are returned in the form "<key>\t<value>"
        # map.mapper takes key, value as two separate arguments
        map_result.extend(map_func(line_parts[0], line_parts[2]))
#        print "map_result" , map_result
    map_result.sort()
    return map_result
    

    
# the user must define their own partition function (hash_func)
def partition(map_result):
    key_value = {}

    # generate a key-value dict
    # ! could use simple dict([]) here, but duplicate keys get overwritten
    for kv_pair in map_result:
        if kv_pair[0] in key_value:
            key_value[kv_pair[0]].append(kv_pair[1])
        else:
            key_value[kv_pair[0]] = [kv_pair[1]]

    print "map_result" , map_result
    print
    print "kv_pair", kv_pair
    print
    print "key_value", key_value
    print

    # try to partition objects bashed on their hash
    partition_hash = {}
    for key, values in key_value.iteritems():
        hashcode = hash_func(key)
        
        # the following is always key in index 0, values following
        # this is so we don't lose track of the key if the hash_func is not
        # totally unique/reversible
        values.insert(0,key)
        if hashcode in partition_hash:
            partition_hash[hashcode].append(values)
        else:
            partition_hash[hashcode] = [values]

    # we should have a grouping of elements by hash now
    print "partition hash:", partition_hash
    print

    # attempt to partition elements based on the number of replicas,
    #   first, get all the hash values and sort them
    hash_list = partition_hash.keys()
    hash_list.sort()

    # initialize the peer_data list
    peer_data = []
    for index in range(mycontext['num_peers']):
        peer_data.append({})

    for hashcode in hash_list:
        cur_replica = hashcode % mycontext['num_peers']
        
        for kv_pairs in partition_hash[hashcode]:
            kv = dict([[kv_pairs[0], kv_pairs[1:]]])
            peer_data[cur_replica].update(kv)

    # print out the peer data, just for my reference
    index = 0
    print "peer_data:"
    for peer in peer_data:
        print index, "->", peer
        index += 1

    # send data to replicas, each replica should have a dictionary of kv pairs  
    for peer_index in range(mycontext['num_peers']):
        address = mycontext['peers'][peer_index].partition(":")
        socketobj = openconn(address[0], int(address[2]))
        send_message_dict(socketobj, peer_data[peer_index])

    
def do_reduce():
    #data = open("reduce_data.dat", "r")
    data = mycontext['reduce_data']
    
    reduce_result = []
    for line in data:
        line_parts = line.partition('\t')
        # I assume that results are returned in the form "<key>\t<value>"
        # reduce.reducer takes key, value as two separate arguments
        reduce_result.append(reduce_func(line_parts[0], line_parts[2]))
                          
    return reduce_result.sort()



# TODO...
def report_results(map_results):
    pass
    

    
def send_message(socketobj, data):
    data = str(len(data)) + "*" + data
    socketobj.send(data)



def send_message_dict(socketobj, data_dict):
    buf = ""
    for key,values in data_dict.iteritems():
        buf += str(key) + "\n"
        for value in values:
            buf += str(value) + "\n"
        
        # after last value, add another return to close key
        buf += "\n"

    send_message(socketobj, buf)



def recv_message_dict(socketobj, initialread=2):
    serialized_dict = recv_message(socketobj, initialread)

    data_dict = {} 
    
    cur_key = ""
    for line in serialized_dict.split("\n"):
        if cur_key == "":
            cur_key = line
            data_dict[cur_key] = []
        elif line == "":
            cur_key = ""
        else:
            data_dict[cur_key].add(line)

    return data_dict



def recv_message(socketobj, initialread=2):
    buf = ""
    found = False
    
    while not found:
        buf += socketobj.recv(initialread)
        if "*" in buf:
            found = True

    parts = buf.partition("*")
    data_len = int(parts[0])
    
    data = parts[2]
    data += socketobj.recv(data_len - len(parts[2]))
    return data



if callfunc == 'initialize':
    mycontext['num_mappers'] = 1
    mycontext['num_reducers'] = 1
    mycontext['state'] = 'Ready'
    
    if len(callargs) > 1:
        raise Exception("too many args")
    elif len(callargs) == 1:
        port = int(callargs[0])
        ip = getmyip()
    else:
        port = 12345
        ip = '127.0.0.1'

    mycontext['myip'] = ip
    mycontext['myport'] = port
    
    print "waiting for connection on ", ip, ":",  port
    listencommhandle = waitforconn(ip, port, get_data)
    
    # block until we've been initialized with data/methods
    while mycontext['state'] == 'Ready':
        sleep(.1)
    
    # try to open connections to all hosts
    mycontext['connection_lock'] = getlock()
    mycontext['peer_conn_left'] = mycontext['num_peers']

    # listen for connections from peers
    listen_replica_init = waitforconn(ip, port, recv_init_replica_conn)    
    # open connections to peers
    init_replica_sockets()

    while mycontext['peer_conn_left'] != 0:
        sleep(0.1)

    
    # stop listening to initialization connections
    #stopcomm(listen_replica_init)
    mycontext['state'] = 'Connected'

    # start listening for map data from peers
    # !! CAN'T DO IT LIKE THIS: use the sockettimeout thing?
    listencommhandle = waitforconn(ip, port, recv_peer_map_data)

    # start mapping, synchronous call
    map_result = do_map()

    # send map results to all reducers, split as necessary
    partition(map_result)
    
    # block until 
    while mycontext['state'] != 'ReducerWaiting':
        sleep(0.1)

    # start reducing, synchronous call (wait for all data to come in, 
    # then start)
    reduce_result = do_reduce()

    report_results(reduce_result)



    
